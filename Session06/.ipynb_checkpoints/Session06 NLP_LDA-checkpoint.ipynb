{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서의 집합 documents\n",
    "documents = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#조건부 확률 분포 정의를 위한 준비\n",
    "\n",
    "#topic의 개수\n",
    "K = 4\n",
    "\n",
    "#1. 각 토픽이 각 문서에 할당되는 횟수\n",
    "#counter로 구성된 list\n",
    "#각각의 counter는 각 문서를 의미함\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "#2. 각 단어가 각 토픽에 할당되는 횟수\n",
    "# 각각의 counter는 각 토픽을 의미함\n",
    "topic_word_counts = [Counter() for _ in range(K)] \n",
    "\n",
    "#3. 각 토픽에 할당되는 총 단어 수\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)] \n",
    "\n",
    "#4. 각 문서에 포함되는 총 단어의 수\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "#5. 단어 종류의 수\n",
    "distinct_words = set(word for document in documents for word in document) \n",
    "W = len(distinct_words)\n",
    "\n",
    "#6. 총 문서의 수 \n",
    "D = len(documents)\n",
    "\n",
    "#### 새로운 topic 계산하기\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "#랜덤으로 생성된 weight로부터 인덱스를 생성함\n",
    "def sample_from(weights):\n",
    "     total = sum(weights)\n",
    "     rnd = total * random.random()       # uniform between 0 and total\n",
    "     for i, w in enumerate(weights):\n",
    "         rnd -= w                        # return the smallest i such that\n",
    "         if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "#topic의 개수\n",
    "K = 4\n",
    "\n",
    "# 각 단어를 임의의 토픽에 배정\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "# 랜덤 초기화한 상태에서 AB를 구하는 데 필요한 숫자 계산하기\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건부 확률 분포를 이용하여 (토픽-단어), (문서-토픽)에 대한 깁스 샘플링 실행하기\n",
    "for iter in range(1000): \n",
    "    for d in range(D): \n",
    "        for i, (word, topic) in enumerate(zip(documents[d], \n",
    "                                              document_topics[d])): \n",
    " \n",
    " \n",
    "           # remove this word / topic from the counts\n",
    "           # so that it doesn't influence the weights \n",
    "            document_topic_counts[d][topic] -= 1 \n",
    "            topic_word_counts[topic][word] -= 1 \n",
    "            topic_counts[topic] -= 1 \n",
    "            document_lengths[d] -= 1 \n",
    " \n",
    "           # choose a new topic based on the weights \n",
    "            new_topic = choose_new_topic(d, word) \n",
    "            document_topics[d][i] = new_topic \n",
    "\n",
    " \n",
    "           # and now add it back to the counts \n",
    "            document_topic_counts[d][new_topic] += 1 \n",
    "            topic_word_counts[new_topic][word] += 1 \n",
    "            topic_counts[new_topic] += 1 \n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n"
     ]
    }
   ],
   "source": [
    "#각 토픽에 가장 영향력이 높은 (weight)값이 큰 단어 탐색\n",
    "for k, word_counts in enumerate(topic_word_counts): \n",
    "         for word, count in word_counts.most_common(): \n",
    "             if count > 0: print (k, word, count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big data and programming languages 7\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "python and statistics 5\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "python and statistics 2\n",
      "databases 2\n",
      "machine learning 2\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "machine learning 3\n",
      "databases 2\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "python and statistics 2\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "databases 3\n",
      "Big data and programming languages 3\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "machine learning 3\n",
      "databases 1\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "python and statistics 2\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "python and statistics 3\n",
      "Big data and programming languages 1\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big data and programming languages 4\n",
      "['statistics', 'R', 'statsmodels']\n",
      "machine learning 3\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "machine learning 3\n",
      "Big data and programming languages 1\n",
      "['pandas', 'R', 'Python']\n",
      "machine learning 3\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "python and statistics 5\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "databases 3\n"
     ]
    }
   ],
   "source": [
    "topic_names = [\"Big data and programming languages\",\n",
    "                    \"python and statistics\",\n",
    "                    \"databases\",\n",
    "                    \"machine learning\"]\n",
    "\n",
    "for document, topic_counts in zip(documents, document_topic_counts): \n",
    "         print (document) \n",
    "         for topic, count in topic_counts.most_common(): \n",
    "             if count > 0: \n",
    "                 print (topic_names[topic], count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
